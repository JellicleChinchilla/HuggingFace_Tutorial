{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "680a91e4-d33f-414a-94bd-bb9031dae219",
   "metadata": {},
   "source": [
    "# Hugging Face Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26db35-20ba-4926-9e60-92b4b3df86c1",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a9a772-d4e2-4df8-95fb-a58315f0f724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:169: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Melo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ca915e-fd37-4cc3-8fe8-6e8c5580b0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.999642014503479}]\n"
     ]
    }
   ],
   "source": [
    "res = classifier(\"I have two cute chinchillas\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe4a8f6-49f9-4676-aeec-7a4e1a8d1c68",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "904b84d8-fba1-47cc-bb20-64ce79e58ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model = \"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "505f729a-35b5-4c38-8d6e-012cf5d706bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"I have two female chinchillas, one of them is grey and the other one is silver, I love them so much. But, I don't want to tell you how much it's worth to have a chinchilla with a white piece\"}]\n"
     ]
    }
   ],
   "source": [
    "res = generator(\n",
    "    \"I have two female chinchillas, one of them is grey and the other one is silver, I love them so much\",\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935bd00a-d485-4197-952e-f3d8aec31d67",
   "metadata": {},
   "source": [
    "## Zero-shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e38e5818-678c-4cee-a5ea-b3c21e58e0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c0b2139-8093-4a8b-a9d3-61555ce98289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'My grandparents raised me up, I love them', 'labels': ['family', 'relationship', 'computation'], 'scores': [0.6796436309814453, 0.313169002532959, 0.007187368348240852]}\n"
     ]
    }
   ],
   "source": [
    "res = classifier(\n",
    "    \"My grandparents raised me up, I love them\",\n",
    "    candidate_labels=[\"computation\", \"family\", \"relationship\"],\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5058dc5-c260-4abb-96f4-e4f09e39dab0",
   "metadata": {},
   "source": [
    "# LangChain: Pipeline -> APIs -> Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136abd5d-18ee-4056-8ac3-0fb0673cd5a1",
   "metadata": {},
   "source": [
    "## Image to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e6ddcae-5e28-46a4-bc1a-f1eb01e22b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "C:\\Users\\Melo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1128: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a family smiling and laughing together\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a family smiling and laughing together'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def img2text(url):\n",
    "    image_to_text = pipeline(\"image-to-text\", model = \"Salesforce/blip-image-captioning-base\")\n",
    "    \n",
    "    text = image_to_text(url)[0][\"generated_text\"]\n",
    "    \n",
    "    print(text)\n",
    "    return text\n",
    "\n",
    "scenario = img2text(\"fam_img.png\")\n",
    "scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f11a43-606e-49ed-8161-45799d87111e",
   "metadata": {},
   "source": [
    "## Translation and Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32707775-233f-4154-85fa-32fb512767c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "820f1c90-d3ce-496a-a51d-945bd42ba25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stroy(scenario):\n",
    "    \n",
    "    template = \"\"\"\n",
    "    你是一位腹有诗书的文学家，以下Context中的内容是一段英文，请以这段英文文本为故事的开端，续写出一个中文故事，请发挥想象力，故事不超过100个汉字。\n",
    "    \n",
    "    CONTEXT: {SCENARIO}\n",
    "    story:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(template = template, input_variables = [\"scenario\"])\n",
    "    \n",
    "    story_llm = LLMChain(llm=OpenAI(\n",
    "        model_name = \"gpt-3.5-turbo\", temperature=1), prompt=prompt, verbose=True)\n",
    "    \n",
    "    story = story_llm.predict(scenario=scenario)\n",
    "    \n",
    "    print(story)\n",
    "    return story\n",
    "\n",
    "\n",
    "# OpenAI API key is needed\n",
    "#generate_stroy(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "657ec74e-2b9e-4d7f-8fd4-68eee8ca1868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1128: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a family smiling and laughing together\n"
     ]
    }
   ],
   "source": [
    "scenario = img2text(\"fam_img.png\")\n",
    "\n",
    "# OpenAI API key is needed\n",
    "#generate_stroy(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf09e59-f61e-4e62-94e4-5dc8aa0ee886",
   "metadata": {},
   "source": [
    "## Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fec3112-1c56-4edb-92b3-1f87efd61891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cannot process without the OpenAI API key for now\n",
    "\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
